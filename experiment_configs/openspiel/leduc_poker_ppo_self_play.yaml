leduc_poker_ppo_self_play:
    run: SELF_PLAY
    stop:
        self_play_rounds: 20
    checkpoint_at_end: True
    num_samples: 5
    config:
        alg: PPO
        # === Self Play ===
        symmetric: False
        self_play_round_stop: 
            training_iteration: 100
        self_play_pretrain_stop:
            training_iteration: 0
        # === Evaluation ===
        population:
          - path: populations/leduc_poker_ppo
            alg: SIMULTANEOUS_PLAY
            mapping: [[1, learned_policy_1]]
        random_eval: True
        multiagent_eval_interval: 20
        # === Environment ===
        horizon: 200
        env: openspiel
        env_config:
            game: leduc_poker
        # === PPO ===
        lambda: 0.95
        gamma: 0.95
        entropy_coeff: 0.001
        clip_param: 0.1
        lr: 0.001
        num_sgd_iter: 8
        train_batch_size: 400
        rollout_fragment_length: 100
        batch_mode: truncate_episodes